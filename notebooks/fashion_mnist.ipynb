{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.data.mnist import MNISTDataModule\n",
    "from torch import nn\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import torchmetrics"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "dm = MNISTDataModule(batch_size=16)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, a baseline model for comparison"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "class BaselineModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(784, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10),\n",
    "        )\n",
    "\n",
    "        self.accuracy = torchmetrics.Accuracy()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.model(x.view(-1, 784))\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x.view(-1, 784))\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x.view(-1, 784))\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log('val_acc_step', self.accuracy(y_hat, y))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_epoch_end(self, outs):\n",
    "\n",
    "        self.log(\"val_acc_epoch\", self.accuracy.compute(), prog_bar=True)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.01)\n",
    "\n",
    "baseline_model = BaselineModel()\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'pl' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/b2/59jjzwlx2m78ct45dplq50jh0000gn/T/ipykernel_74534/540163335.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBaselineModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLightningModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pl' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "trainer = pl.Trainer(max_epochs=5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "trainer.fit(model=baseline_model, datamodule=dm)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/soren/Repositories/Speciale/.venv/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n",
      "\n",
      "  | Name     | Type       | Params\n",
      "----------------------------------------\n",
      "0 | model    | Sequential | 136 K \n",
      "1 | accuracy | Accuracy   | 0     \n",
      "----------------------------------------\n",
      "136 K     Trainable params\n",
      "0         Non-trainable params\n",
      "136 K     Total params\n",
      "0.544     Total estimated model params size (MB)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 4: 100%|██████████| 3751/3751 [00:30<00:00, 121.67it/s, loss=0.196, v_num=6, val_loss=0.247, val_acc_step=0.947, val_acc_epoch=0.937]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generative process:\n",
    "\n",
    "1. Draw $\\bm \\theta \\sim \\mathcal N (0, \\alpha^2 I)$\n",
    "2. Draw $\\bm y \\sim Dist_y(NN(x;\\theta))$ (For whatever observation model, in this case Dirichlet)\n",
    "\n",
    "\n",
    "$$p(\\bm y, \\theta | \\alpha) = p(\\theta|0, \\alpha^2 I) \\prod_{n=1}^N p(y_n | \\theta, X) $$ \n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, alpha=0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.ffnn = nn.Sequential(\n",
    "                nn.Linear(784, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, 10),\n",
    "            )\n",
    "\n",
    "    def posterior(self, x):\n",
    "\n",
    "        x = self.ffnn(x.view(-1, 784))\n",
    "        return torch.distributions.Categorical(x.exp())\n",
    "\n",
    "    def log_joint_prob(self, batch):\n",
    "\n",
    "        x, y = batch\n",
    "\n",
    "        lp = sum(param_prior.log_prob(x).sum() for x in self.parameters())\n",
    "        lp +=self.posterior(x).log_prob(y).sum()\n",
    "\n",
    "        return lp\n",
    "\n",
    "model = Model()\n",
    "batch = next(iter(dm.train_dataloader()))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "source": [
    "model.log_joint_prob(batch)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(181635.7656, grad_fn=<AddBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "source": [
    "posterior.log_prob(y)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([-2.2923, -2.3584, -2.2544, -2.3829, -2.2565, -2.3082, -2.2628, -2.2470,\n",
       "        -2.2318, -2.3020, -2.2992, -2.2804, -2.3048, -2.2823, -2.3020, -2.2854],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "y"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([5, 6, 4, 9, 5, 2, 8, 5, 5, 7, 4, 3, 1, 3, 2, 7])"
      ]
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "param_prior = torch.distributions.Normal(0, 0.1)\n",
    "\n",
    "# computes log joint probability of the model (y, X, lambda and sigma are assume to be fixed/given)\n",
    "def log_joint_prob():\n",
    "    # log probability of the prior\n",
    "    \n",
    "    \n",
    "    # log likelihood\n",
    "    res = model(x)\n",
    "        \n",
    "    return lp\n",
    "\n",
    "# a few tests checks\n",
    "log_joint_prob(parameters)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "log_joint_prob() takes 0 positional arguments but 1 was given",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/b2/59jjzwlx2m78ct45dplq50jh0000gn/T/ipykernel_64927/3171896709.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# a few tests checks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mlog_joint_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: log_joint_prob() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('.venv': venv)"
  },
  "interpreter": {
   "hash": "d767b8386092d7cf7d2b9cae3bbc1311fb6c0ac52cead8f221dab1753f4d4fb8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}