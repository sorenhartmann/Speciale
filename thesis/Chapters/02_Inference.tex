\chapter{Inference}

Data is $\mathcal{D} = (\bm{X}, \bm{Y})$, where $\bm{X} = (x_1,\dots,x_N)$ are covariates and $\bm{Y} = (y_1, \dots, y_N)$ are attributes we want to model.
Given new data $x^\ast$, find the predictive distribution $p(y^\ast | x^\ast)$ 

%TODO: Why not $p(y^\ast | x^\ast, \mathcal{D})$?

Marginalize out the parameters of the model, $\theta$.
\begin{align*}
    p(y^\ast | x^\ast) &= E_{p(\theta|\mathcal{D})} p(y^\ast | x^\ast, \theta) \\
                       &= \int_{\theta} p(y^\ast | x^\ast, \theta)  p(\theta|\mathcal{D}) d\theta
\end{align*}
In practice, at least in the case of deep learning models, the integral of the expectation feasable to evalutate exactly. 
We therefore resort to approximate methods, such as the Monte Carlo estimate:
\begin{align*}
    p(y^\ast | x^\ast)  \approx \frac{1}{N} \sum_{j=1}^N p(y^\ast | x^\ast, \theta^{(i)}) 
\end{align*}
Where $\theta^{(i)}$ are samples from the posterior distribution:
\begin{align*}
    p(\theta | \mathcal{D}) 
    &= p(\theta | \bm{X}, \bm{Y}) \\
    &= \frac{p(\bm{Y} |\theta, \bm{X}) p(\theta|\bm{X})}{p(\bm{Y}) } \\
    &= \frac
    {\left(\prod_{i=1}^N p(y_i |\theta, x_i)\right) p(\theta)}
    {p(\bm{Y}) }  
\end{align*}
The unconditional  observation model, $p(y)$, is also not easily evalutated. 
In the following, we will discuss two different strategies to get around this. 
The first is to use sampling methods that only requires the sampling distribution $f(\theta)$ up to a constant factor $\hat{f}(\theta)\propto f(\theta)$. 
This is useful since 
\begin{align*}
    p(\theta | \mathcal{D}) 
    &= \frac
    {\left(\prod_{i=1}^N p(y_i |\theta, x_i)\right) p(\theta)}
    {p(\bm{Y}) }  \\
    &\propto \left(\prod_{i=1}^N p(y_i |\theta, x_i)\right) p(\theta)
\end{align*}
and both the likelihood, $p(y_i |\theta, x_i)$ and the prior $p(\theta)$ are specified as part of the graphical model, at least up to a constant factor. 
The other approach is to use an approximate posterior $q(\theta)$ in place of the exact posterior. 

The different methods are applicable not only for sampling from a posterior $p(\theta | \mathcal{D})$, but from sampling from any random variable with a density function $p(x)$ speficied up to a constant factor.

\section{Markov Chain Monte Carlo}
Suppose we want to sample from a target distribution $p(x)$, yet we only know $\tilde{p}(x) = 1/K\cdot p(x)$, where $K$ is constant in $x$.
The Markov Chain Monte Carlo (MCMC) methods use a Markov chain with transition to generate samples sequentially from $p(x)$. 
We achieve this by ensuring that the Markov chain with transition density $q(x^\prime | x)$ satisfies two requirements
% TODO: maybe a bit informal?
\begin{itemize}
    \item The target distribution $p(x)$ stationary with respect to the Markov chain, that is:
    \begin{align} \label{eq:stationarity}
        p(x^\prime) = \int_{\Omega} q(x^\prime| x) p(x) \dd{x} 
    \end{align}
    \item The Markov chain eventually reaches the stationary distribution $p(x)$. This is satisfied by ensuring the markov chain is irreducible and aperiodic.
    %TODO: Explain terms?
\end{itemize}
A sufficent criteria for $p(x)$ to be stationary with respect to the Markov chain is that the transition probablities $q(x^\prime|x)$ satisfy the property of detailed balance:
\begin{align*}
    p(x)q(x^\prime | x) = p(x^\prime)q(x| x^\prime).
\end{align*}
This implies stationary since
\begin{align*}
    \int_{\Omega} q(x^\prime | x) p(x) \dd{x} 
    &= \int_{\Omega}  p(x^\prime)q(x| x^\prime) \dd{x} \\ 
    &=  p(x^\prime) \int_{\Omega} q(x| x^\prime) \dd{x} \\
    &=  p(x^\prime) \cdot 1 \\
    &= p(x^\prime).
\end{align*}

The following chapters discusses a straigh forward method applying the property of detailed balance.

\subsection{Metropolis-Hastings}

\question{Er lidt i tvivl om hvorledes transition probablities skal fortolkes i forbindelse med kontinuære state spaces. Det meste litteratur jeg kan finde kigger kun på endelige eller tællelige state space, men vil det ikke stort set altid være kontinuære state spaces vi kigger på i forhold til MH/HMC?}

Suppose at some step $t$, the Markov chain has state $x^{(t)}$. The Metropolis-Hastings algorithm uses a symmetric proposal distribution $g(x|x^{(t)})$ to generate a new sample, $x^\prime$. The sample is then \emph{accepted} with probability:
\begin{align*}
    A(x^{\prime}, x^{(t)}) = \min\left\{1, \frac{\tilde{p}(x^\prime)}{\tilde{p}(x^{(t)})}\right\}
\end{align*}
If the sample is accepted, $x^{(t+1)} \gets x^\prime$. Otherwise, the new state is rejected and $x^{(t+1)} \gets x^{(t)}$. This process is then repeated until we have obtained the required amount of samples $x^{(1)},\dots,x^{(N)}$. 

%TODO: Transition probability vs density...
Suppose at some point, the state of the chain is $x$. The transition density to some other state $x^\prime$ is then:
\begin{align*}
    q(x^\prime|x) = g(x^\prime | x) A(x^\prime, x)
\end{align*}
That is, the probability that state $x^\prime$ is suggested given state $x$, and that the proposed state $x^\prime$ is accepted. We can verify that this transition probability does indeed satisfy the property of detailed balance:
\begin{align*}
    p(x)q(x^\prime|x) &= p(x)g(x^\prime | x) A(x^\prime, x) \\
                      &= p(x)g(x^\prime | x) \min\left\{1,  \tilde{p}(x^\prime) / \tilde{p}(x)\right\} \\
                      &= \underbrace{p(x)}_{\geq 0} \underbrace{g(x^\prime | x)}_{=g(x | x^\prime)} \min\left\{1,  p(x^\prime) / p(x)\right\} \\
                      &= g(x | x^\prime) \min\left\{p(x), p(x^\prime)\right\} \\
                      &= g(x | x^\prime) p(x^\prime) \min\left\{p(x)/p(x^\prime), 1\right\} \\
                      &= p(x^\prime) g(x | x^\prime)  A(x ,x^\prime) = p(x^\prime) q(x|x^\prime).
\end{align*}
Thus, the Markov chain satisfies the property of detailed balance, and is stationary with respect to the Markov chain. If we choose our proposal distribution such that the chain is also irreducible and aperiodic, eg. by having $q(x^\prime|x) \neq 0$ for all $x^\prime,x$, the sample distribution should therefore eventually converge towards $p(x)$. 

While the Metropolis-Hastings algorithm will eventually sample from the correct distribution, it is not given that this necessarily occurs within a reasonable time. 
We are essentially hoping that our proposal distribution $g(x^\prime | x)$ reliably doesn't propose new states $x^\prime$ with much lower probability, $p(x^\prime) \ll p(x) $, in order to keep the acceptance probability $A(x^\prime, x)$ at reasonable levels. 
For simple problems, this can often be done by tuning a Gaussian distribution around the current state, however for problems with high dimensionality and complex target distributions, this strategy may fall short.

\subsection{Gibbs sampling}

Suppose we had some way of easily sampling each of our variables conditioned on our other variables, that is $p(x_i|\bm{x}_{\setminus i})$. 
The Gibbs sampling strategy generates a new sample $\bm{x}$ by sampling each variable $x_i$ in turn from $p(x_i|\bm{x}_{\setminus i})$.  
First we need to show that the joint distribution $p(\bm{x})$ is invariant to each of these steps. 
In order to argue for the stationarity of this procedure, we argue that each of the steps is invariant to the target distribution.
\question{For meget abuse of notation? Helt galt afmarcheret?}

The transition density $q(x^\prime| x)$ can be specified using the Dirac Delta function as:
\begin{align*}
    q(x^\prime| x) = \delta_{x_{\setminus i}}(x^\prime_{\setminus i}) p(x_i^\prime|x_{\setminus i})
\end{align*}
\begin{align*}
    \int_{\Omega} q(x^\prime| x) p(x) \dd{x}  
    &= \int_{\Omega}  \delta_{x_{\setminus i}}(x^\prime_{\setminus i}) p(x_i^\prime|x_{\setminus i}) p(x_i,x_{\setminus i}) \dd{x_{\setminus i}} \dd{x_i} \\
    &= \int_{\Omega} p(x^\prime_i|x^\prime_{\setminus i}) p(x_i,x^\prime_{\setminus i}) \dd{x_i} \\ 
    &= p(x^\prime_i|x^\prime_{\setminus i}) \int_{\Omega}  p(x_i,x^\prime_{\setminus i}) \dd{x_i} \\ 
    &= p(x^\prime_i|x^\prime_{\setminus i}) \int_{\Omega}  p(x_i | x^\prime_{\setminus i}) p(x^\prime_{\setminus i}) \dd{x_i} \\ 
    &= p(x^\prime_i|x^\prime_{\setminus i}) p(x^\prime_{\setminus i}) \int_{\Omega}  p(x_i | x^\prime_{\setminus i})  \dd{x_i} \\ 
    &= p(x^\prime_i|x^\prime_{\setminus i}) p(x^\prime_{\setminus i}) \cdot 1 \\
    &=  p(x^\prime_i,x^\prime_{\setminus i})  \\
    &= p(x^\prime)
\end{align*}

If the condtional distributions $p(x_i|x_{\setminus i})$ are non-zero for all values of all variables $x_i$, the chain is also ergodic. If not, as discussed in \cite{bishop_pattern_2006}, we would have to prove the ergodicity of the sampling strategy explicitly.

\subsection{Hamiltonian Monte Carlo}

A popular method to improve the proposal strategy is the Hamiltonian Monte Carlo (HMC) algorithm. 
From the Metropolis-Hastings algorithm we see that we need a proposal strategy, that both leaves the target distribution invariants and while being able to take as large steps as possible in order to explore the parameter space.
Borrowing from the field of dynamical systems, HMC uses the notion of conservation of kinetic and potential energy to generate samples from the target distribution. 
Specifically, the variables are modelled as an Hamiltonian system. To this end, we define the \emph{momentum}, $r_i$ of each variable $x_i$.
\begin{align*}
    \dv{x_i}{t} = M^{-1} r_i
\end{align*}
Where $M$ is the mass matrix of the system. Then, the kinetic energy, $K$ of the particle can be described as a function of the momentum:
\begin{align*}
    K(r) = \frac{1}{2}r^T M^{-1} r.
\end{align*}
The rate of change of the momentum, ie. the acceleration is given by the negative gradient of the potential energy, $E$:
\begin{align*}
    \dv{r_i}{t} = -\pdv{E(x)}{x_i}
\end{align*}
Introducing the Hamiltonian function $H(x, r) = K(r) + E(x)$, we can write the dynamics governing the system as
\begin{align*}
    \dv{x_i}{t} &= \phantom{-}\pdv{H}{r_i}\\
    \dv{r_i}{t} &= -\pdv{H}{x_i}
\end{align*}
Since $\pdv{H}{r_i} = \pdv{K}{r_i} = M^{-1}r_i$ and $-\pdv{H}{x} = -\pdv{E}{x_i}$. During evolution of such a system, the value of $H$ stays constant for any particle. 
Another property of this system is that volume is also preserved under evolution. 
Next we consider sampling from the joint density:
\begin{align} \label{eq:hmc-joint}
    p(x, r) = \frac{1}{C_H} \exp\left[ -H(x, r)\right] = \frac{1}{C_H} \exp\left[-K(r) - E(x)  \right].
\end{align}
If we throw away the samples from $r$, we are then left with samples from the marginal distribution of $x$. We then introduce the negative log density as potential energy of the particle:
\begin{align*}
    E(x) = -\log{p(x)}.
\end{align*}
Looking at \cref{eq:hmc-joint}, we see that under the distribution $p(x, r)$, the variables $x$ and $r$ are independent, and the marginal distribution of $x$ indeed factors out to be the target distribution $p(x)$. 

\todo[inline]{1. Resampling af momentum \\
2. Vis invariants/stationaritet og ergodicitet af diskretiseret system
}


% Hver gang vi tager et skridt i tid er p(x, z) det samme, samt det område det dækkker   p(x, z) er alstå invariant overfor 

% Men ikke ergodic... fixes med gibbs step på p(z|x) som er normalfordelt(0, M).


% Ayy see \cref{alg:1}
% \begin{figure}[htbp]
%     \centering
%     \begin{minipage}{.7\linewidth}
%       \begin{algorithm}[H]
%         \caption{Hey} \label{alg:1}
%         \begin{algorithmic}
%             \Require $n \geq 0$
%             \Ensure $y = x^n$
%             \State $y \gets 1$
%             \State $X \gets x$
%             \State $N \gets n$
%             \While{$N \neq 0$}
%             \If{$N$ is even}
%                 \State $X \gets X \times X$
%                 \State $N \gets \frac{N}{2}$  \Comment{This is a comment}
%             \ElsIf{$N$ is odd}
%                 \State $y \gets y \times X$
%                 \State $N \gets N - 1$
%             \EndIf
%             \EndWhile
%             \end{algorithmic}
%       \end{algorithm}
%     \end{minipage}
%   \end{figure}
    
\subsection{Stochastic Gradient HMC}

In cases where we want to sample from the the posterior of some parameters $\theta$,  $p(\theta | \mathcal{D})$ for some dataset $\mathcal{D}$, it may be infeasable to calculate the gradient $\nabla E(\theta) = \nabla p(\theta) - \log{p(\bm{Y} | \theta, \bm{X})}$. 
This is often the case for deep learning applications.
Typically, in the maximum likelihood setting, this problem is handled by use of stochastic gradient descent algorithm such as ADAM. 
These methods approximate the gradient based only on a subset of the data. 
We may consider a similar scheme, approximating the $E$ gradient as:
\begin{align*}
    \nabla\tilde{E}(\theta) = -\left(\frac{|\mathcal{D}|}{|\tilde{\mathcal{D}}|}  \nabla \sum_{x_i, y_i\in \tilde{\mathcal{D}}} \log{p(y_i |\theta, x_i)}\right)  -\nabla \log{p(\theta)}
\end{align*}

% TODO: Rigor? Central estimate?
In \cite{chen_stochastic_2014}, they model the behavior of this approach with:
\begin{align*}
    \nabla\tilde{E}(\theta) \approx \nabla{E}(\theta) + e
\end{align*}
where $e \sim \mathcal{N}(0, V(\theta))$ is the noise introduced through the approximation. This has some ramification for the Hamiltonian system described in the previous section. 
The discrete time step updates performed by the algorithm no longer corresponds to the deterministic Hamiltonian system because of the introduced noise. 
This is not in it of itself a problem, as we are already correcting for discretization errors through the Metropolis-Hastings step, and we could in principle correct for the gradient noise in the same fashion.
The problem lies in the fact that we would have to evaluate the Hamiltonian for the entire dataset, in order to perform the MH step.

In order to figure out how to get around this, we take a further look at the, now stochastic, dynamics of the system. If we for some step size $\epsilon$ consider the discretizated system as
\begin{align*}
\Delta r =  -\epsilon\nabla{\tilde{E}}(\theta) = -\epsilon\nabla{E}(\theta)  + \epsilon e,
\end{align*}

The corresponding continuous dynamical system is then 

\begin{align*}
    \dd{r} = -\epsilon\nabla{E}(\theta)\dd{t}  + \dd{e_t},
\end{align*}
    
\question{I paperet bruger de notationen $\nabla\tilde{E}(\theta) = \nabla{E}(\theta) + \mathcal{N}(0, V(\theta))$, som jeg ikke helt kan gennemskue hvordan jeg skal bruge ift. at identificere fokker planck ligningen. Ideen var at bruge at skrive den op tættere på ligningen præsenteret her \url{https://en.wikipedia.org/wiki/Fokker–Planck_equation}, men mangler vist at reparameterisere støj termen, eftersom de dér bruger en standard Weiner process.}
% what are the dynamics of the corresponding continuous system as $\epsilon \to 0$? Consider $w \sim \mathcal{N}(0, \epsilon)$, the increment can then be reparameterized as:
% \begin{align*}
%     \Delta r  = -\epsilon\nabla{E}(\theta)  + \sqrt{\epsilon} A(\theta) w.
% \end{align*}
% Where $A(\theta)A(\theta)^T = V(\theta)$.
% From the parameterization above, we can consider the continuous system
% \begin{align*}
%     \dd r  = -\nabla{E}(\theta) \dd t  + \sqrt{\epsilon} A(\theta) \dd w.
% \end{align*}
% Letting (some of?) $\epsilon $....

% With $B(\theta) = \frac{1}{2}\epsilon V(\theta)$

% We get 

% \begin{align*}
%     d\begin{bmatrix}
%         \theta \\
%         r
%     \end{bmatrix} = - \begin{bmatrix}
%         0 & -1 \\ 
%         1 &  B 
%     \end{bmatrix}
% \end{align*}




% \begin{align*}
% \end{align*}




% Time step update becomes
% \begin{align*}
%     \Delta x &= \epsilon M^{-1} r \\
%     \Delta r &= -\epsilon\nabla\tilde{E}(\theta) 
%     = -\epsilon\nabla{E}(\theta)  -\epsilon \mathcal{N}(0, V(\theta))
%     = -\epsilon\nabla{E}(\theta)  +\mathcal{N}(0, \epsilon^2 V(\theta)) \\
% \end{align*}

% \begin{align*}
%     \Delta \theta &= \epsilon M^{-1} r \\
%     \Delta r &=  -\epsilon\nabla{E}(\theta) + \mathcal{N}(0, \epsilon^2 V(\theta)) \\
% \end{align*}


% Alternatively, we write the $r$ update as, $V(\theta) = S$:



% \begin{align*}
%     \Delta \theta &= \epsilon M^{-1} r \\
%     \Delta r &=  -\epsilon\nabla{E}(\theta) + V(\theta) \mathcal{N}(0, \epsilon) \\
% \end{align*}


% \section{Variational Methods}

% \begin{align*}
%     \hat V_{x\in \mathcal D}[\nabla U(\theta)_{i,i} ] = \frac{1}{|\tilde{\mathcal{D}}|-1} \sum_{x\in \tilde{\mathcal{D}}} (\nabla U_x(\theta)_{i,i} - \hat{E}[\nabla U(\theta)_{i,i}])^2
% \end{align*}

% \begin{align*}
%     \hat V_{x\in \mathcal D}[\nabla U(\theta)_{i,i} ] = 
% \end{align*}