\chapter{Inference}

Data is $\mathcal{D} = (\bm{X}, \bm{Y})$, where $\bm{X} = (x_1,\dots,x_N)$ are covariates and $\bm{Y} = (y_1, \dots, y_N)$ are attributes we want to model.
Given new data $x^\ast$, find the predictive distribution $p(y^\ast | x^\ast)$ 

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}
    
        % Define nodes
        \node[latent] (theta) {$\theta$};
        \node[obs, right=of theta] (targets) {$y_i$};
        \node[obs, right=of targets] (covariates) {$x_i$};
        
        % Connect the nodes
        \edge {theta} {targets}
        \edge {covariates} {targets}
    
        % \edge {eta, covariates} {targets};
        
        % Plates
        % \plate {parameters} {(eta)} {$I\times J$};
        % \plate {layers} {(lambda)(parameters)} {$K$};
        \plate {observations} {(targets)(covariates)} {$|\mathcal{D}|$};
    
    \end{tikzpicture}
    \caption{PGM of the BNN model.}
\end{figure}

The model yields the following factorization:
\begin{align*}
    p(\theta, \bm X, \bm Y) = p(\theta) p(\bm X) p(\bm Y | \bm X, \theta)
\end{align*}
Marginalize out the parameters of the model, $\theta$.
\begin{align*}
    p(y^\ast | x^\ast) &= E_{p(\theta|\mathcal{D})} p(y^\ast | x^\ast, \theta) \\
                       &= \int_{\theta} p(y^\ast | x^\ast, \theta)  p(\theta|\mathcal{D}) d\theta
\end{align*}
As
In practice, at least in the case of deep learning models, the integral of the expectation feasable to evalutate exactly. 
We therefore resort to approximate methods, such as the Monte Carlo estimate:
\begin{align*}
    p(y^\ast | x^\ast)  \approx \frac{1}{N} \sum_{j=1}^N p(y^\ast | x^\ast, \theta^{(i)}) 
\end{align*}
Where $\theta^{(i)}$ are samples from the posterior distribution:
\begin{align*}
    p(\theta | \mathcal{D}) 
    &= \frac{p(\bm Y , \bm X, \theta)}{p(\bm{X}, \bm{Y})}\\
    &= \frac{p(\theta) p(\bm X) p(\bm Y | \bm X, \theta)}{p(\bm{Y}|\bm{X})p(\bm{X})} \\
    &= \frac{p(\theta) p(\bm Y | \bm X, \theta)}{p(\bm{Y}|\bm{X})}
\end{align*}

\begin{align}
    p(\bm{X}, \bm{Y}, \theta) = p(\bm{Y} | \bm{X}, \theta) p(\bm{X}, \theta) \\
    p(\bm{X}, \bm{Y}, \theta) = p(\bm{X}, \bm{Y} | \theta) p(\theta) 
\end{align}
The unconditional observation model, $p(Y|X)$, . 
In the following, we will discuss two different strategies to get around this. 
The first is to use sampling methods that only requires the sampling distribution $f(\theta)$ up to a constant factor $\hat{f}(\theta)\propto f(\theta)$. 
This is useful since 
\begin{align*}
    p(\theta | \mathcal{D}) 
    &= \frac
    {\left(\prod_{i=1}^N p(y_i |\theta, x_i)\right) p(\theta)}
    {p(\bm{Y}) }  \\
    &\propto \left(\prod_{i=1}^N p(y_i |\theta, x_i)\right) p(\theta)
\end{align*}
and both the likelihood, $p(y_i |\theta, x_i)$ and the prior $p(\theta)$ are specified as part of the graphical model, at least up to a constant factor. 
The other approach is to use an approximate posterior $q(\theta)$ in place of the exact posterior. 

The different methods are applicable not only for sampling from a posterior $p(\theta | \mathcal{D})$, but from sampling from any random variable with a density function $p(x)$ speficied up to a constant factor.

\section{Markov Chain Monte Carlo}
Suppose we want to sample from a target distribution $p(x)$, yet we only know $\tilde{p}(x) = 1/K\cdot p(x)$, where $K$ is constant in $x$.
The Markov Chain Monte Carlo (MCMC) methods use a Markov chain with transition to generate samples sequentially from $p(x)$. 
We achieve this by ensuring that the Markov chain with transition density $q(x^\prime | x)$ satisfies two requirements

\begin{itemize}
    \item The target distribution $p(x)$ stationary with respect to the Markov chain, that is:
    \begin{align} \label{eq:stationarity}
        p(x^\prime) = \int_{\Omega} q(x^\prime| x) p(x) \dd{x} 
    \end{align}
    \item The Markov chain eventually reaches the stationary distribution $p(x)$. This is satisfied by ensuring the markov chain is irreducible and aperiodic.

\end{itemize}
A sufficent criteria for $p(x)$ to be stationary with respect to the Markov chain is that the transition probablities $q(x^\prime|x)$ satisfy the property of detailed balance:
\begin{align*}
    p(x)q(x^\prime | x) = p(x^\prime)q(x| x^\prime).
\end{align*}
This implies stationary since
\begin{align*}
    \int_{\Omega} q(x^\prime | x) p(x) \dd{x} 
    &= \int_{\Omega}  p(x^\prime)q(x| x^\prime) \dd{x} \\ 
    &=  p(x^\prime) \int_{\Omega} q(x| x^\prime) \dd{x} \\
    &=  p(x^\prime) \cdot 1 \\
    &= p(x^\prime).
\end{align*}

The following chapters discusses a straigh forward method applying the property of detailed balance.

\subsection{Metropolis-Hastings}

Suppose at some step $t$, the Markov chain has state $x^{(t)}$. The Metropolis-Hastings algorithm uses a symmetric proposal distribution $g(x|x^{(t)})$ to generate a new sample, $x^\prime$. The sample is then \emph{accepted} with probability:
\begin{align*}
    A(x^{\prime}, x^{(t)}) = \min\left\{1, \frac{\tilde{p}(x^\prime)}{\tilde{p}(x^{(t)})}\right\}
\end{align*}
If the sample is accepted, $x^{(t+1)} \gets x^\prime$. Otherwise, the new state is rejected and $x^{(t+1)} \gets x^{(t)}$. This process is then repeated until we have obtained the required amount of samples $x^{(1)},\dots,x^{(N)}$. 


The transition density $q(x_{k+1}|x_k)$ is such that 
\begin{align}
    P(x_{k+1} \in B | x_k ) = \int_B q (x_{k+1}|x_k )\dd x_{k+1}
\end{align}

Using the proposal distribution we generate a sample $x^\ast_{k+1}$.The next sample $x_{k+1}$ is then within some $B\subset \Omega$, if and only if $x^\ast_{k+1}\in A$ and the sample is accepted, ie.
\begin{equation}
    \begin{aligned}
        P(x_{k+1} \in B | x_k ) 
        &= \int_\Omega P( x_{k+1} \in B | x^\ast_{k+1},x_k) g(x^\ast_{k+1}|x_k) \dd{x^\ast_{k+1}}\\
        &= \int_B P( x_{k+1} \in B | x^\ast_{k+1},x_k) g(x^\ast_{k+1}|x_k) \dd{x^\ast_{k+1}} \\
        &= \int_B A(x^\ast_{k+1}, x_k) g(x^\ast_{k+1}|x_k) \dd{x^\ast_{k+1}}
    \end{aligned}
\end{equation}
We thus see that the transition density of this sampling scheme is
\begin{align}
    q(x^\prime|x) = g(x^\prime | x) A(x^\prime, x)
\end{align} 
Suppose at some point, the state of the chain is $x$. The transition density to some other state $x^\prime$ is then:
\begin{align*}
    q(x^\prime|x) = g(x^\prime | x) A(x^\prime, x)
\end{align*}
We can verify that this transition probability does indeed satisfy the property of detailed balance:
\begin{align*}
    p(x)q(x^\prime|x) &= p(x)g(x^\prime | x) A(x^\prime, x) \\
                      &= p(x)g(x^\prime | x) \min\left\{1,  \tilde{p}(x^\prime) / \tilde{p}(x)\right\} \\
                      &= \underbrace{p(x)}_{\geq 0} \underbrace{g(x^\prime | x)}_{=g(x | x^\prime)} \min\left\{1,  p(x^\prime) / p(x)\right\} \\
                      &= g(x | x^\prime) \min\left\{p(x), p(x^\prime)\right\} \\
                      &= g(x | x^\prime) p(x^\prime) \min\left\{p(x)/p(x^\prime), 1\right\} \\
                      &= p(x^\prime) g(x | x^\prime)  A(x ,x^\prime) = p(x^\prime) q(x|x^\prime).
\end{align*}
Thus, the Markov chain satisfies the property of detailed balance, and is stationary with respect to the Markov chain. If we choose our proposal distribution such that the chain is also irreducible and aperiodic, eg. by having $q(x^\prime|x) \neq 0$ for all $x^\prime,x$, the sample distribution should therefore eventually converge towards $p(x)$. 

While the Metropolis-Hastings algorithm will eventually sample from the correct distribution, it is not given that this necessarily occurs within a reasonable time. 
We are essentially hoping that our proposal distribution $g(x^\prime | x)$ reliably doesn't propose new states $x^\prime$ with much lower probability, $p(x^\prime) \ll p(x) $, in order to keep the acceptance probability $A(x^\prime, x)$ at reasonable levels. 
For simple problems, this can often be done by tuning a Gaussian distribution around the current state, however for problems with high dimensionality and complex target distributions, this strategy may fall short.

\subsection{Gibbs sampling}

Suppose we had some way of easily sampling each of our variables conditioned on our other variables, that is $p(x_i|\bm{x}_{\setminus i})$. 
The Gibbs sampling strategy generates a new sample $\bm{x}$ by sampling each variable $x_i$ in turn from $p(x_i|\bm{x}_{\setminus i})$.  
First we need to show that the joint distribution $p(\bm{x})$ is invariant to each of these steps. 
In order to argue for the stationarity of this procedure, we argue that each of the steps is invariant to the target distribution.

The transition density $q(x^\prime| x)$ can be specified using the Dirac Delta function as:
\begin{align*}
    q(x^\prime| x) = \delta_{x_{\setminus i}}(x^\prime_{\setminus i}) p(x_i^\prime|x_{\setminus i})
\end{align*}

We can verify that this transition density does inseed satisfy the stationarity requirements in \cref{eq:stationarity}.
\begin{align*}
    \int_{\Omega} q(x^\prime| x) p(x) \dd{x}  
    &= \int_{\Omega}  \delta_{x_{\setminus i}}(x^\prime_{\setminus i}) p(x_i^\prime|x_{\setminus i}) p(x_i,x_{\setminus i}) \dd{x_{\setminus i}} \dd{x_i} \\
    &= \int_{\Omega} p(x^\prime_i|x^\prime_{\setminus i}) p(x_i,x^\prime_{\setminus i}) \dd{x_i} \\ 
    &= p(x^\prime_i|x^\prime_{\setminus i}) \int_{\Omega}  p(x_i,x^\prime_{\setminus i}) \dd{x_i} \\ 
    &= p(x^\prime_i|x^\prime_{\setminus i}) \int_{\Omega}  p(x_i | x^\prime_{\setminus i}) p(x^\prime_{\setminus i}) \dd{x_i} \\ 
    &= p(x^\prime_i|x^\prime_{\setminus i}) p(x^\prime_{\setminus i}) \int_{\Omega}  p(x_i | x^\prime_{\setminus i})  \dd{x_i} \\ 
    &= p(x^\prime_i|x^\prime_{\setminus i}) p(x^\prime_{\setminus i}) \cdot 1 \\
    &=  p(x^\prime_i,x^\prime_{\setminus i})  \\
    &= p(x^\prime)
\end{align*}

If the condtional distributions $p(x_i|x_{\setminus i})$ are non-zero for all values of all variables $x_i$, the chain is also ergodic. If not, as discussed in \cite{bishop_pattern_2006}, ergodicity of the sampling strategy will have to be proven explicitly.

\subsection{Hamiltonian Monte Carlo}

A popular method to improve the proposal strategy is the Hamiltonian Monte Carlo (HMC) algorithm. 
From the Metropolis-Hastings algorithm we see that we need a proposal strategy, that both leaves the target distribution invariants and while being able to take as large steps as possible in order to explore the parameter space.
Borrowing from the field of dynamical systems, HMC uses the notion of conservation of kinetic and potential energy to generate samples from the target distribution. 
Specifically, the variables are modelled as an Hamiltonian system. To this end, we define the \emph{momentum}, $r_i$ of each variable $x_i$.
\begin{align*}
    \dv{x_i}{t} = M^{-1} r_i
\end{align*}
Where $M$ is the mass matrix of the system. Then the kinetic energy, $K$ of the particle can be described as a function of the momentum:
\begin{align*}
    K(r) = \frac{1}{2}r^T M^{-1} r.
\end{align*}
The rate of change of the momentum, ie. the acceleration is given by the negative gradient of the potential energy, $E$:
\begin{align*}
    \dv{r_i}{t} = -\pdv{E(x)}{x_i}
\end{align*}
Introducing the Hamiltonian function $H(x, r) = K(r) + E(x)$, we can write the dynamics governing the system as
\begin{equation}\label{eq:hmc-dynamics}
    \begin{aligned}
        \dv{x_i}{t} &= \phantom{-}\pdv{H}{r_i}\\
        \dv{r_i}{t} &= -\pdv{H}{x_i}
    \end{aligned}
\end{equation}
Since $\pdv{H}{r_i} = \pdv{K}{r_i} = M^{-1}r_i$ and $-\pdv{H}{x} = -\pdv{E}{x_i}$. 
Such a system is called a Hamiltonian system, and the useful property that values of $H$ remain constant along trajectories of the system.
We then consider sampling from the joint density:
\begin{align} \label{eq:hmc-joint}
    p(x, r) = \frac{1}{C_H} \exp\left[ -H(x, r)\right] = \frac{1}{C_H} \exp\left[-K(r) - E(x)  \right].
\end{align}
If we throw away the samples from $r$, we are then left with samples from the marginal distribution of $x$. If we then introduce the negative log density as potential energy of the particle,
\begin{align*}
    E(x) = -\log{p(x)}
\end{align*}
looking at \cref{eq:hmc-joint}, we see that under the distribution $p(x, r)$, the variables $x$ and $r$ are independent, and the marginal distribution of $x$ indeed factors out to be the target distribution $p(x)$. 

\newcommand{\newx}{x^{\prime}}
\newcommand{\newr}{{r^{\prime}}}
\newcommand{\oldx}{{x^{(k)}}}
\newcommand{\oldr}{{r^{(k)}}}
\newcommand{\nextx}{x^{(k+1)}}
\newcommand{\nextr}{{r^{(k+1)}}}

Ideally we would sample from $p(x, r)$ in two steps. Suppose the current position in phase space is $(\oldx, \oldr)$. 
First, the momentum is resampled as $\oldr^\ast\sim \mathcal{N}(0, M^{-1})$. Then, the dynamics of the system is simulated for some amount of time $\Delta$ to yield the next sample $(\nextx, \nextr) =\phi_\Delta(\oldx, \oldr^\ast)$.
% Let $\phi(x, r, t):$ be the dynamical system defined in \cref{eq:hmc-dynamics}, 

The stationarity of this procedure is shown by showing the stationarity of each of the two steps. From \cref{eq:hmc-joint}, we see that $p(r|x) \propto \exp[-K(r)] = \exp[\frac{1}{2}r^tM^{-1}r]$, ie. the conditional distribution of $r$ is $\mathcal{N}(0, M^{-1})$.
By resampling the momentum, we are thus performing a Gibbs step, and as seen previously, this does indeed have $p(x, y)$ as stationary distribution.
\question{$\delta(z-a)$ vs $\delta_a(z)$? Nemmere måde at vise det på?}
Then, since we are deterministically simulating the system, we can model the next transition density using the delta function, $q(z^\prime|z) = \delta(z^\prime - \phi_\Delta(z))$, where $z=(x, r)$. 
Since $H$ is constant along trajectories of the system, $p(z) =p(\phi_t(z))$ for any $t$, and thus
\begin{align} \label{eq:}
    \int_{\Omega} q(z^\prime| z ) p(z) \dd{z} = \int_{\Omega} \delta(z^\prime - \phi_\Delta(z))p(z) \dd{z} 
    = p(\phi_{-\Delta}(z^\prime)) = p(z^\prime).
\end{align}
The second step therefore also satisfies the stationarity requirement. The ergodicity of the sampling procedure is ensured by the initial resampling of the procedure. Since we are sampling the momentum from a normal distribution, there is a non zero probablity of reaching any point in phase space for each sample.

In practice, it is not possible to exactly simulate the system, and we therefore make use of a leapfrog integrator, $\hat{\phi}_\epsilon^T$, simulating the dynamics using $T$ discrete time steps of size $\epsilon$. While we can obtain very good approximations, these also come with a computational cost, and would still be susceptible to areas of high curvature potentially leading to unstable estimates. 

Instead, we use a Metropolis-Hastings step to correct for the introduced discretization error. First, we ensure that the proposal distribution is symmetric. This can be achieved by using an additional step $N(x, r) = (x, -r)$, negating the momentum after simulating the dynamics. The proposed sample then becomes $ N(\hat{\phi}_\epsilon^T(x, r))= (x^\ast, r^\ast)$. This is useful since by repeating the process, we end up exactly where we started, ie. $ N(\hat{\phi}_\epsilon^T(x^\ast, r^\ast)) = (x, r)$. In order to show this, we define some functions describing the leapfrog algorithm:
\begin{align*}
    S \left(\begin{bmatrix} x \\ r \end{bmatrix}\right) 
    &= \begin{bmatrix} x + \epsilon r M^{-1} \\ r - \epsilon \nabla E(x + \epsilon r M^{-1}) \end{bmatrix} &
    U\left(\begin{bmatrix} x \\ r \end{bmatrix}\right) 
    &= \begin{bmatrix} x \\ r - \frac{\epsilon}{2} \nabla E(x) \end{bmatrix}, \\
    V\left(\begin{bmatrix} x \\ r \end{bmatrix}\right) 
    &= \begin{bmatrix} x \\ r + \frac{\epsilon}{2} \nabla E(x) \end{bmatrix} &
    N \left(\begin{bmatrix} x \\ r \end{bmatrix}\right) 
    &= \begin{bmatrix} x \\ -r
    \end{bmatrix} 
\end{align*}
The integrator for step size $\epsilon$, using $T$ step can then be defined as the following map:
\begin{align*}
    \phi^T_\epsilon = V\circ S^{\circ T} \circ U
\end{align*}
Now, we want to show that for any $z=(x, r)$, we get $N(\hat{\phi}^T_\epsilon(N(\hat{\phi}^T_\epsilon(z))) = z$. First, we show the case for $T=1$, which is simply a matter of evaulating the functions and simplifying.
\begin{align*}
    N (\hat{\phi}^1_\epsilon(N(\hat{\phi}^1_\epsilon(z))) 
    &=
    (N \circ \hat{\phi}^1_\epsilon \circ N \circ \hat{\phi}^1_\epsilon)(z) \\
    &= (N \circ V \circ S \circ U \circ N \circ V \circ S \circ U )\left( 
    \begin{bmatrix}
        x \\ r
    \end{bmatrix}
    \right) \\
    &= (N\circ V \circ S \circ U \circ N \circ V\circ S) \left(
    \begin{bmatrix}
        x \\ r - \frac{\epsilon}{2} \nabla E(x)
    \end{bmatrix}
    \right) \\
    &= (N\circ V \circ S \circ U \circ N \circ V )\left(
    \begin{bmatrix}
        x + \epsilon (r - \frac{\epsilon}{2} \nabla E(x)) M^{-1} \\ 
        r - \frac{\epsilon}{2} \nabla E(x) - \epsilon \nabla E(x + \epsilon (r - \frac{\epsilon}{2} \nabla E(x)) M^{-1})
    \end{bmatrix}
    \right) \\
    &= (N\circ V \circ S \circ U \circ N )\left(
    \begin{bmatrix}
        x + \epsilon (r - \frac{\epsilon}{2} \nabla E(x)) M^{-1} \\ 
        r - \frac{\epsilon}{2} \nabla E(x) - \frac{\epsilon}{2} \nabla E(x + \epsilon (r - \frac{\epsilon}{2} \nabla E(x)) M^{-1})
    \end{bmatrix}
    \right) \\
    &= (N\circ V \circ S \circ U )\left(
    \begin{bmatrix}
        x + \epsilon (r - \frac{\epsilon}{2} \nabla E(x)) M^{-1} \\ 
        -r + \frac{\epsilon}{2} \nabla E(x) + \frac{\epsilon}{2} \nabla E(x + \epsilon (r - \frac{\epsilon}{2} \nabla E(x)) M^{-1})
    \end{bmatrix}
    \right) \\
    &= (N\circ V \circ S)\left(
    \begin{bmatrix}
        x + \epsilon (r - \frac{\epsilon}{2} \nabla E(x)) M^{-1} \\ 
        -r + \frac{\epsilon}{2} \nabla E(x)
    \end{bmatrix}
    \right)\\
    &= (N\circ V)
    \left(
    \begin{bmatrix}
        x  \\ 
        -r - \frac{\epsilon}{2} \nabla E(x)
    \end{bmatrix}
    \right) \\
    &= N\left(\begin{bmatrix}
        x  \\ 
        -r
    \end{bmatrix}\right)\\
    &= \begin{bmatrix}
        x  \\ 
        r
    \end{bmatrix}
\end{align*}

Assuming that $N(\hat{\phi}^{T+1}_\epsilon(N(\hat{\phi}^{T+1}_\epsilon(z)))) = z$, we then show by induction that this is the case for any $T$. We thus need to show that
\begin{align*}
    N(\hat{\phi}^{T+1}_\epsilon(N(\hat{\phi}^{T+1}_\epsilon(z)))) &=z
\end{align*}
From the definitions, we see that the following identities must hold 
\begin{align*}
    V \circ U = U\circ V = N \circ N=\text{id} 
\end{align*}
This also implies that under the induction hypothesis $\hat{\phi}^{T+1}_\epsilon(N(\hat{\phi}^{T+1}_\epsilon(z))) = N(z)$, and consequently:
\begin{equation}
    \begin{aligned}
        N\circ V\circ S^{\circ T+1} \circ U \circ N \circ V \circ S^{\circ T+1} \circ U
        &= N\circ V\circ S\circ S^{\circ T} \circ U \circ N \circ V \circ  S^{\circ T} \circ S \circ U  \\
        &= N\circ V\circ S\circ U\circ 
        \underbrace{V \circ S^{\circ T} \circ U \circ N \circ V \circ  S^{\circ T} \circ U}_{\text{$=N$ per i.h.}}
        \circ V\circ S \circ U \\
        &= N\circ V\circ S\circ U\circ N \circ V\circ S \circ U \\
        &= \text{id} 
    \end{aligned}
\end{equation}
And so $N(\hat{\phi}^{T+1}_\epsilon(N(\hat{\phi}^{T+1}_\epsilon(z)))) = z$, and indeed $N(\hat{\phi}^{T}_\epsilon(N(\hat{\phi}^{T}_\epsilon(z)))) = z$ for any number of steps $T$. 

\question{Area preservation and stuff... }
The proposal distribution $g(z^\prime | z) = \delta(z^\prime- \hat{\phi}^{T}_\epsilon(z)))$ is therefore symmetrical. Including a Metropolis-Hastings step, accepting the proposed state with probability
\begin{align}
    \min\left\{1, \frac{p(z)}{p(z^\prime)}\right\} = \min\left\{ 1, \exp[-E(x) + E(x^\prime)] \right\}.
\end{align}
Since we are resampling the momentum immediately after each integration step, and $K(r) = K(-r)$ we don't need to perform the negation of the momentum in practice. We thus end up with the algorithm described in \cref{alg:hmc}.

\algnewcommand{\IfThenElse}[3]{% \IfThenElse{<if>}{<then>}{<else>}
  \State \algorithmicif\ #1\ \algorithmicthen\ #2\ \algorithmicelse\ #3}
\algnewcommand{\LineComment}[1]{\State \(\triangleright\) \textit{#1}}

\begin{algorithm}[H]
\caption{Hamiltonian Monte Carlo} \label{alg:hmc}
\begin{algorithmic}
    \Require $n \geq 0$
    \For{$n=1,2,\dots$}
    \State resample momentum $r \sim \mathcal{N}(0, M^{-1})$
    \State $x \gets x_{t-1}$
    \State $r \gets r - \epsilon \nabla E(x)/2$ \Comment{Leapfrog integrator}
    \For{$i=1,\dots,T$} 
        \State $x\gets x + \epsilon r M^{-1} $ 
        \IfThenElse{$i < N$}{$r\gets r - \epsilon \nabla E(x)$}{$r\gets r - \epsilon \nabla E(x)/2$}
    \EndFor
    \State sample $u \sim \mathcal{U}(0, 1)$
    \If{$u \leq \exp[-E(x_t) + E(x)]$}
        \State $x_{t} \gets x$
    \Else
        \State $x_{t} \gets x_{t-1}$
    \EndIf
    \EndFor
    \end{algorithmic}
\end{algorithm}


% \begin{align}
%     x_0, r_0 &= x, r\\
%     r_1 &= r_0 - \epsilon \nabla E(x_0) / 2 \\
%     x_i &= x_{i-1}+ \epsilon r_{i} M^{-1},\quad i=1,\dots,n \\
%     r_i &= r_{i-1}- \epsilon \nabla E(x_{i-1}),\quad i=2,\dots,n-1 \\
%     r((i+1/2)\epsilon) &= r((i-1/2)\epsilon) - \epsilon \nabla E(x(i\epsilon)) \\
% \end{align}


% Hver gang vi tager et skridt i tid er p(x, z) det samme, samt det område det dækkker   p(x, z) er alstå invariant overfor 

% Men ikke ergodic... fixes med gibbs step på p(z|x) som er normalfordelt(0, M).


    
\subsection{Stochastic Gradient HMC}

In cases where we want to sample from the the posterior of some parameters $\theta$,  $p(\theta | \mathcal{D})$ for some dataset $\mathcal{D}$, it may be infeasable to calculate the gradient $\nabla E(\theta) = \nabla p(\theta) - \log{p(\bm{Y} | \theta, \bm{X})}$. 
This is often the case for deep learning applications.
Typically, in the maximum likelihood setting, this problem is handled by use of stochastic gradient descent algorithm such as ADAM. 
These methods approximate the gradient based only on a subset of the data. 
We may consider a similar scheme, approximating the $E$ gradient as:
\begin{align*}
    \nabla\tilde{E}(\theta) = -\left(\frac{|\mathcal{D}|}{|\tilde{\mathcal{D}}|}  \nabla \sum_{x_i, y_i\in \tilde{\mathcal{D}}} \log{p(y_i |\theta, x_i)}\right)  -\nabla \log{p(\theta)}
\end{align*}

In \cite{chen_stochastic_2014}, they model the behavior of this approach with:
\begin{align} \label{eq:sghmc-model}
    \nabla\tilde{E}(\theta) \approx \nabla{E}(\theta) + e
\end{align}
where $e \sim \mathcal{N}(0, \sigma^2(\theta))$ is the noise introduced through the approximation. This has some ramification for the Hamiltonian system described in the previous section. 
The discrete time step updates performed by the algorithm no longer corresponds to the deterministic Hamiltonian system because of the introduced noise. 

While we may try to use the HMC algorithm anyway, we could also attempt to modify the dynamics of the system with this noise term in mind.
If we for some step size $\epsilon$ consider the discretizated system as
\begin{align}
    \Delta \theta &=  \epsilon M^{-1} r\\
    \Delta r &=  -\epsilon\nabla{\tilde{E}}(\theta) = -\epsilon\nabla{E}(\theta)  + \epsilon e
\end{align}
These updates can be interpreted as discrete steps with size $\epsilon$ from the continuous stochastic system:
\begin{align*}
    \dd{\theta} &= M^{-1} r\dd{t} \\
    \dd{r} &= -\nabla{E}(\theta)\dd{t}  + \sqrt{\epsilon}\sigma(\theta) \dd{W_t} 
\end{align*}
Where $W_t$ is a $k$ dimensional Weiner process. Alternatively, we can express the dynamics as a $2k$-dimensional system using vector notation:
\begin{align} \label{eq:stoch-hmc-naive}
    \dd{\begin{bmatrix}\theta \\ r \end{bmatrix}} &= 
    \begin{bmatrix}
        M^{-1}r \\ -\nabla{E}(\theta) 
    \end{bmatrix} \dd{t} + \begin{bmatrix}
        0 & 0 \\ 
        0 & \sqrt{\epsilon}\sigma(\theta)
    \end{bmatrix} \dd{W_t}
\end{align}
\question{$\sigma^2(\theta)$ vs $\sigma(\theta)$}
As argued in \cite{chen_stochastic_2014}, the target distribution $p(\theta, r)$ is indeed no longer stationary with respect to the now stochastic dynamics of \cref{eq:stoch-hmc-naive}. Thus, unlike the original Hamiltonian dynamics, where we only had to correct for the discretization error, in this case not even the underlying dynamics are stationary with respect to $p(\theta, r)$. Assuming we know the noise model $\sigma^2(\theta)$, the solution proposed in \cite{chen_stochastic_2014} is to instead introduce a friction term in \cref{eq:stoch-hmc-naive}:
\begin{align} \label{eq:stoch-hmc-w-friction}
    \dd{\begin{bmatrix}\theta \\ r \end{bmatrix}} &= 
    \begin{bmatrix}
        M^{-1}r \\ -\nabla{E}(\theta)-\frac{1}{2}\epsilon\sigma^2(\theta)M^{-1}r
    \end{bmatrix} \dd{t} + \begin{bmatrix}
        0 & 0 \\ 
        0 & \sqrt{\epsilon}\sigma(\theta)
    \end{bmatrix} \dd{W_t}
\end{align}
We may rewrite the system  as:
\begin{align*}
    \dd{\begin{bmatrix}\theta \\ r \end{bmatrix}} &= 
    -\begin{bmatrix}
        0 & -1 \\ 
        1 & \frac{1}{2}\epsilon\sigma^2(\theta)
    \end{bmatrix} \begin{bmatrix}
        \nabla{E}(\theta)  \\ M^{-1}r
    \end{bmatrix}\dd{t} + \begin{bmatrix}
        0 & 0 \\ 
        0 & \sqrt{\epsilon}\sigma(\theta)
    \end{bmatrix} \dd{W_t} 
\end{align*}
Introducing the matrices
\begin{align} \label{eq:stoch-hmc-matrix-defs}
    G=\begin{bmatrix}0 & -1 \\ 1 & 0 \end{bmatrix}, ~D = BB^T= \begin{bmatrix}
        0 & 0 \\ 
        0 & \frac{1}{2}\epsilon\sigma^2(\theta)
    \end{bmatrix},
\end{align}
and identifying the gradient of the Hamiltonian, we can rewrite the system further as 
\begin{align}
    \dd{\begin{bmatrix}\theta \\ r\end{bmatrix}}= - (D+G) \nabla H(\theta, r)\dd{t} + \sqrt{2}B \dd{W_t}.
\end{align}
We can then make use of the Fokker-Planck equations \cite{ottinger_stochastic_1996}, to describe the evolution of our target distribution $p(\theta, r)=p(z)$ under these dynamics. According the FP equation, for the stochastic system:
\begin{align}
    \dd{X_t} =A(t, X_t) + S(t, X_t)\dd{W_t}
\end{align}
with $V=SS^T$, we then have
\begin{align*}
    \pdv{t} p(t, z) &= - \sum_{i} \pdv{z_i} A_i(t, z) p(t, z) 
        + \frac{1}{2}\sum_{i}\sum_{j}\pdv{}{z_i}{z_j}V_{i,j}(t, z)p(t, z). 
\end{align*}
In our case $A = -(D+G)\nabla H(\theta, r)$, and $S = \sqrt{2}B$. Inserting into the equation with $p_t(z) = \exp[-H(z)]$above, we find that
\begin{align} \label{eq:stoch-hmc-fp}
    \pdv{t} p_t(z) &= - \sum_{i} \pdv{z_i} [ -(D+G)\nabla H(\theta, r)]_i p_t(z) 
        + \frac{1}{2}\sum_{i}\sum_{j}\pdv{}{z_i}{z_j}[2D]_{i,j}p_t(z). 
\end{align}
From \cref{eq:stoch-hmc-matrix-defs}, we see that $D(\theta)$ only depends on $z = (r, \theta)$, in the lower right quadrant, where the terms depends on $\theta$. Since the partial derivatives are only taken with respect to the momentum varibles, $r$ in this quadrant, we can reduce the second term:
\begin{align*}
    \frac{1}{2}\sum_{i}\sum_{j}\pdv{}{z_i}{z_j}[2D]_{i,j}p_t(z)
    &= \frac{1}{2}\sum_{i}\pdv{z_i}\sum_{j}\pdv{z_j}\underbrace{[2D]_{i,j}}_{\text{Const. wrt. $z_j$}}p_t(z) \\
    &= \frac{1}{2}\sum_{i}\pdv{z_i}\sum_{j}[2D]_{i,j}\pdv{z_j}p_t(z)\\
    &= \frac{1}{2}\sum_{i}\pdv{z_i}[2D \nabla p_t(z)]_{i}
\end{align*}
We can then insert into the expression in \cref{eq:stoch-hmc-fp}:
\begin{align*} 
    \pdv{t} p_t(z) &= - \sum_{i} \pdv{z_i} [ -(D+G)\nabla H(z)]_i p_t(z) 
        + \frac{1}{2}\sum_{i}\sum_{j}\pdv{}{z_i}{z_j}[2D]_{i,j}p_t(z). \\
    &=  \sum_{i} \pdv{z_i} [ (D+G)\nabla H(z)p_t(z)]_i 
        + \frac{1}{2}\sum_{i}\pdv{z_i}[2D \nabla p_t(z)]_{i} \\
    &=  \sum_{i} \pdv{z_i} \left([ (D+G)\nabla H(z)p_t(z)]_i 
        + \frac{1}{2}[2D \nabla p_t(z)]_{i}\right) \\
    &=  \sum_{i} \pdv{z_i} \left([ (D+G)\nabla H(z)p_t(z) 
        + D \nabla p_t(z)]_{i}\right) \\
    &=  \sum_{i} \pdv{z_i} \left([ (D+G)\nabla H(z)p_t(z) 
        - D \nabla H(z) p_t(z)]_{i}\right) \\
    &=  \sum_{i} \pdv{z_i} \left([ G\nabla H(z)p_t(z)]_i \right) 
\end{align*}
Where we have used that $\nabla p_t(z) = \nabla \exp[-H(z)]= -\nabla H(z) \exp[-H(z)] =  -\nabla H(z) p_t(z)$. 
\begin{align}
    \pdv{t} p_t(z) &= \sum_{i} \pdv{z_i} [ G\nabla H(z)p_t(z)]_i \\
                &= \sum_{i} \pdv{z_i} [ -G\nabla p_t(z)]_i \\
                &= \sum_{\theta_i} \pdv{\theta_i} [ -G\nabla p_t(z)]_{\theta_i}
                + \sum_{r_i} \pdv{r_i} [ -G\nabla p_t(z)]_{r_i} \\
                % &= \sum_{\theta_i} \pdv{\theta_i} [ -G\nabla p_t(z)]_{\theta_i}
                % + \sum_{r_i} \pdv{r_i} [ -G\nabla p_t(z)]_{r_i} \\
    % \pdv{t} p_t(z) &= 
    % \sum_{\theta_i} \pdv{\theta_i} \left([ G\nabla H(z)p_t(z)]_i \right) +
    % \sum_{r_i} \pdv{r_i} \left([ G\nabla H(z)p_t(z)]_i \right)
\end{align}
Next, since 
\begin{align*}
    -G\nabla p_t(z) = \begin{bmatrix}0 & 1 \\ -1 & 0 \end{bmatrix}
    \begin{bmatrix}
        \nabla_\theta p_t(z) \\ 
        \nabla_r p_t(z)
    \end{bmatrix} = \begin{bmatrix}
        \nabla_r p_t(z) \\
        -\nabla_\theta p_t(z) 
    \end{bmatrix}
\end{align*}
We can conclude that $[-G\nabla p_t(z)]_{\theta_i} = [\nabla p_t(z)]_{r_i} = \pdv*{p_t(z)}{r_i}$, and likewise $[-G\nabla p_t(z)]_{r_i} = [-\nabla p_t(z)]_{\theta_i} =-\pdv*{p_t(z)}{\theta_i}$. We can thus simplify further
\begin{align*}
    \pdv{t} p_t(z) 
    &= \sum_{\theta_i} \pdv{\theta_i} [ -G\nabla p_t(z)]_{\theta_i}
        + \sum_{r_i} \pdv{r_i} [ -G\nabla p_t(z)]_{r_i} \\
    &= \sum_{\theta_i} \pdv{\theta_i} \pdv{r_i} p_t(z)
        - \sum_{r_i} \pdv{r_i} \pdv{\theta_i} p_t(z)\\
    &= 0
\end{align*}
Summarizing, we see that the time derivative of the distribution $p_t(z)=\exp[-H(z)]$ is zero, and the distribution thus remains the same under the dynamics of the stochastic system in \cref{eq:stoch-hmc-w-friction}. 
There is still some considerations when it comes to implementing this approach in practice. 

First, the noise model $\sigma$ is only approximately normal, and is generally not known.
We instead use an estimate $\hat{B}$ for the dispersion matrix, which also introduces error into the sampling procedure. 
Approximating the noise normal is more reasonable for larger batch sizes with reference to the central limit theorem.
In order to alleviate the estimation error somewhat, we instead using an upper bound $C \succeq 2D$ of the system noise. We then sample

\begin{align}
    \Delta \theta &=  \epsilon M^{-1} r\\
    \Delta r &=  -\epsilon\nabla\tilde{E}(\theta) - \epsilon CM^{-1}r  + \tau
\end{align}

Where $\tau\sim  \mathcal{N}(0, C - 2\hat{D})$. In effect, we are simulating the dynamics of a noisier system by injecting additional noise, however now we atleast know some terms of the noise model. With $C$ much larger than $D$, the relative impact of the estimation noise becomes smaller. 
In \cite{chen_stochastic_2014}, they simply set $\hat{D} = 0$, and relies on this upper bound to appromite the dynamics sufficiently.  

Secondly, we are still stuck with approximating the continuous dynamics using discrete time steps. Unlike the HMC algorithm, we do not have any way of correcting for the discretization error. 
We are therefore still only approximately sampling from the target distrubion, and will have to rely on the step size $\epsilon$ to be small for the approximation to be good. We do not, however, need to resample the momentum, since ergodity is simply provided through the stochastic nature of the algorithm.

Finally in \cite{chen_stochastic_2014} they argue for an alternative parameterization. With $v = \epsilon M^{-1} r$ Then
\begin{align}
    \Delta \theta &=  v \\
    \Delta v &=  -\epsilon^2 M^{-1} \nabla\tilde{E}(\theta) - \epsilon CM^{-1}v  + \epsilon M^{-1}\tau \\
    \Delta v &=  -\epsilon^2 M^{-1} \nabla\tilde{E}(\theta) - \epsilon CM^{-1}v  + \epsilon M^{-1}\tau
\end{align}
where $\epsilon M^{-1}\tau\sim Noget andet$? Using $\eta = \epsilon^2M^{-1}$, $\beta = \epsilon M^{-1} D$ and $\alpha = \epsilon M^{-1}C$, we can parameterize the algorithm with
\begin{align}
    \Delta \theta &=  v \\
    \Delta v &=  -\eta \nabla\tilde{E}(\theta) - \alpha v  + \mathcal{N}(2(\alpha-\hat{\beta})\eta)
\end{align}
We have thus factored the mass matrix $M$ out of the specification of the algorithm, but now specify it in conjuction with the step size $\epsilon$. 
This is useful, since we now don't need to specify the algorithm in terms of parameters with questionable interpretations such as the mass matrix, but the actual behavior of the algorithm.
This parameterization can be seen in \cref{alg:sghmc}

Comparing the algorithm with stocastic gradient descent with momentum, we see that  $\eta$ corresponds to learning rate while $\alpha$ corresponds to momentum decay. 
When is comes to choosing parameters for the methods, we can therefore to some extend draw on experience and/or conventional wisdom.

\begin{algorithm}[H]
    \caption{Stochastic Gradient Hamiltonian Monte Carlo} \label{alg:sghmc}
    \begin{algorithmic}
        \For{$n=1,2,\dots$}
        \For{$i=1,\dots,T$} 
            \State sample $\tau \sim \mathcal{N}(\text{whateverthefuck})$
            \State $x\gets x + v $ 
            \State $v\gets v - \eta \nabla E(x) - \alpha v + \tau$
        \EndFor
        \State $x_{t} \gets x$
        \EndFor
        \end{algorithmic}
    \end{algorithm}

\section{Variational Inference}

Suppose we want to sample from some posterior distribution $p(\theta | \mathcal{D})$. 
Variational methods aims to approximate this posterior distribution using a so called variational distribution $q_\psi (\theta)$ parameterized by some parameters $\psi$.  
We then seek to to find a value of these variational parameters, such that the variational distribution is as close to the posterior as possible. 
As a quantifying measure of difference between the two distributions, we use the Kullback-Liebler divergence:
\begin{align}
    \text{KL}(q_\psi\parallel p) 
    = \int_{\Omega} q_\psi(\theta)\log{\frac{q_\psi(\theta)}{p(\theta|\mathcal{D})}} \dd{\theta} 
    = E_{q_\psi}\left[\log{\frac{q_\psi(\theta)}{p(\theta|\mathcal{D})}}\right]
\end{align}
that is, the inference problem turns into an optimization problem 
\begin{align}
    \psi^\ast=\arg\min_{\psi}{\text{KL}(q_\psi\parallel p)}.
\end{align}
Inserting the parameter posterior from REF into the expression
\begin{align}
    \text{KL}(q_\psi\parallel p) 
    &= \int_{\Omega} q_\psi(\theta)\log{\frac{q_\psi(\theta)}{p(\theta|\mathcal{D})}} \dd{\theta} \\
    &= \int_{\Omega} q_\psi(\theta)\log{\frac{p(\bm{Y}|\bm{X})q_\psi(\theta)}{p(\bm{Y} |\theta, \bm{X}) p(\theta)}} \dd{\theta} \\
    &= \int_{\Omega} q_\psi(\theta)\log{p(\bm Y|\bm{X})} + q_\psi(\theta)\log{\frac{q_\psi(\theta)}{p(\bm{Y} |\theta, \bm{X}) p(\theta)}} \dd{\theta} \\
    &= E_{q_{\psi}} [\log{p(\bm{Y}|\bm{X})}] + \int_{\Omega} q_\psi(\theta)\log{\frac{q_\psi(\theta)}{p(\bm{Y} |\theta, \bm{X}) p(\theta)}} \dd{\theta} 
\end{align}
While we don't know the value of marginalized likelihood $\log{p(\bm{Y}|\bm{X})}$, since the parameters are marginalized, we can ignore the term for the purposes of the minimization problem.
We therefore seek to minimize:
\begin{align}
    \int_{\Omega} q_\psi(\theta)\log{\frac{q_\psi(\theta)}{p(\bm{Y} |\theta, \bm{X}) p(\theta|\bm{X})}} \dd{\theta} 
    &= \int_{\Omega} q_\psi(\theta) \log{\frac{q_\psi(\theta)}{p(\theta|\bm{X})}} -q_\psi(\theta)\log{p(\bm{Y} |\theta, \bm{X})} \dd{\theta}\\
    &= \text{KL}(q_\psi(\theta) \parallel p(\theta))  - E_{q_\psi}\left[ \log{p(\bm{Y} |\theta, \bm{X})}  \right] 
\end{align}
We can therefore see than finding the optimal variational parameters is equivalent to finding the variational parameters maximumizing the expected likelihood while regularized such that it stays close to the prior distribution. 


How may we find derivatives of distributions:
\begin{align*}
    \pdv{\psi} E_{q_\psi(\theta)}\left[ f(\psi, \theta) \right]
\end{align*}
This can be solved by using the reparameterization trick. That is, introducing a parameterless random variable $\epsilon$ following some distribution distribution $s(\epsilon)$, and use a deterministic function $t(\epsilon, \psi)$ such that $t(\epsilon, \psi) \sim q_\psi(\theta)$. Then the expectation 
$ E_{q_\psi(\theta)}\left[ f(\psi, \theta) \right] =E_{s(e)}\left[ f(\psi, t(\epsilon, \psi)) \right]$ and 
\begin{align*}
    \pdv{\psi} E_{q_\psi(\theta)}\left[ f(\psi, \theta) \right] 
    &= \pdv{\psi} \int_\theta q_\psi(\theta) f(\psi, \theta)\dd{\theta}\\
    &= \pdv{\psi} \int_\epsilon s(\epsilon) f(\psi, t(\epsilon, \psi))\dd{e}\\
    &= \int_\epsilon \pdv{\psi} s(\epsilon) f(\psi, t(\epsilon, \psi))\dd{e}\\
    &= \int_\epsilon s(\epsilon) \pdv{\psi} f(\psi, t(\epsilon, \psi)) \dd{e}
\end{align*}
Since $s(\epsilon)$ does not depend on the variational parameters $\psi$, we need only find the partial derivative of $f(\psi, t(\epsilon, \psi))$ using the chain rule:
\begin{align*}
    \pdv{\psi} f(\psi, t(\epsilon, \psi)) 
    &= \pdv{f(\psi, \theta)}{\psi}
    + \pdv{f(\psi, \theta)}{\theta}\pdv{t(\epsilon, \psi)}{\psi} 
\end{align*}
and so 
\begin{align*}
    \pdv{\psi} E_{q_\psi(\theta)}\left[ f(\psi, \theta) \right] 
    &= \int_\epsilon s(\epsilon) \left(\pdv{f(\psi, \theta)}{\psi}
    + \pdv{f(\psi, \theta)}{\theta}\pdv{t(\epsilon, \psi)}{\psi} \right) \dd{e} \\
    &= E_{s(\epsilon)} \left[\pdv{f(\psi, \theta)}{\psi} 
    + \pdv{f(\psi, \theta)}{\theta}\pdv{t(\epsilon, \psi)}{\psi} \right]
\end{align*}
How may we estimate this gradient?  Using 

\begin{align*}
    \frac{1}{N} \sum_{i=1}^N \pdv{f(\psi, \theta_i)}{\psi} 
    + \pdv{f(\psi, \theta)}{\theta}\pdv{t(\epsilon_i, \psi)}{\psi} 
\end{align*}
    



% \begin{align*}
% \end{align*}




% Time step update becomes
% \begin{align*}
%     \Delta x &= \epsilon M^{-1} r \\
%     \Delta r &= -\epsilon\nabla\tilde{E}(\theta) 
%     = -\epsilon\nabla{E}(\theta)  -\epsilon \mathcal{N}(0, V(\theta))
%     = -\epsilon\nabla{E}(\theta)  +\mathcal{N}(0, \epsilon^2 V(\theta)) \\
% \end{align*}

% \begin{align*}
%     \Delta \theta &= \epsilon M^{-1} r \\
%     \Delta r &=  -\epsilon\nabla{E}(\theta) + \mathcal{N}(0, \epsilon^2 V(\theta)) \\
% \end{align*}


% Alternatively, we write the $r$ update as, $V(\theta) = S$:



% \begin{align*}
%     \Delta \theta &= \epsilon M^{-1} r \\
%     \Delta r &=  -\epsilon\nabla{E}(\theta) + V(\theta) \mathcal{N}(0, \epsilon) \\
% \end{align*}


% \section{Variational Methods}

% \begin{align*}
%     \hat V_{x\in \mathcal D}[\nabla U(\theta)_{i,i} ] = \frac{1}{|\tilde{\mathcal{D}}|-1} \sum_{x\in \tilde{\mathcal{D}}} (\nabla U_x(\theta)_{i,i} - \hat{E}[\nabla U(\theta)_{i,i}])^2
% \end{align*}

% \begin{align*}
%     \hat V_{x\in \mathcal D}[\nabla U(\theta)_{i,i} ] = 
% \end{align*}