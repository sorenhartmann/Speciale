\chapter{Discussion}

At the outset of this project, we wanted to investigate how  Bayesian methods apply to deep learning problems to achieve better modeling performance.
Since deep learning applications are often very data-intensive, for feasibility, we would prefer that the methods are applicable for batched learning. 
Primarily focusing on MCMC methods, we see that naively implementing the HMC sampling algorithm leads to incorrect samples in a batching data setting.
We model the noise introduced through batching as a Gaussian distribution and then correct for the additional variance by adding a friction term to the HMC algorithm. 
The resulting sampling algorithm, named SGHMC, closely resembles regular stochastic gradient descent with momentum, with some additional noise added to the gradient, and can be parameterized as such.
For comparison with the MCMC, we also implemented the variational inference algorithm.

We demonstrate the efficacy of the SGHMC algorithms through two simulated experiments. 
First, a small-scale experiment, precisely replicating the noise model, shows that the algorithm can correct for the noise applied the gradient in an ideal scenario. 
Next, a simulated example of Bayesian linear regression demonstrates the algorithm's validity beyond an idealized setting.  
We find that SGHMC, as implemented in \autocite{chen_stochastic_2014}, does yield better samples than a naive HMC implementation. 
Further, we see that introducing a variance estimator to the SGHMC algorithm improves the quality of the samples and specific statistics. 

Now that we have seen that extending the sampling methods to account for a batched learning setting is possible through SGHMC and improves performance in a simulated, small-scale setting, we next look unto into how these methods scale in a deep learning context.

We see that for a medium-sized feed-forward neural network applied to MNIST, the discussed SGHMC algorithms perform worse than a standard SGD approach using dropout. 
Implementing a medium-sized convolutional neural network for the more challenging to predict dataset CIFAR10, the SGHMC algorithms improve performance with and without gradient variance estimation. 
Applying the different methods to the larger convolutional model, DenseNet, we again find that regular SGD yields the best performance in terms of accuracy. 

These results are not entirely surprising.
The feed-forward neural network model is a highly flexible model due to modeling interactions between every pair of pixels in the input images. 
Combined with an uninformative prior on the model parameters, we thus also get a very uninformative prior in function space.
Comparably, the convolutional model prior may be more informative and thus makes the Bayesian MCMC method more effective. 

On the other hand, we also see inferior performance for the DenseNet models. 
It is possible that this is mostly caused by the removal of the batch normalization layers. 
While we also removed the batch normalization layers in the smaller convolutional model, the DenseNet model is much deeper and may therefore be more dependent on the batch normalization layers for efficient training.

As for SGHMC with gradient variance estimation, it is unclear whether this approach is helpful in a deep learning context. 
We see better performance with gradient estimation for the feed-forward network; however, the performance is worse for the convolutional models.
Using the gradient variance estimates seems to calibrate the model slightly better.

However, the above considerations are primarily dependent on the fact that the samples from the sampling algorithms represent the actual posterior of the model parameters. 

We see throughout the training of every deep learning model that the kinetic temperature diagnostic $\hat{p}_{0.99}$ is not satisfied for either sampler. 
This breakdown indicates that both sampling algorithms seemingly break down for every deep learning model to some extent. 
This breakdown may be caused by optimizing the modeling parameters for the validation accuracy, which may not coincide with the most accurate samples from the posterior. 
How much this influences the quality of the samples is unclear, but it does not bode well for the robustness of the methods as implemented.
Therefore, it is also unclear whether we can attribute the superior performance of the medium-sized convolutional model on CIFAR10 to the Bayesian approach or whether it is due to the larger ensemble of models or something else.

There are some ways we may go about trying to achieve a more robust sampling algorithm. 
In \autocite{wenzel_how_2020}, they demonstrate how they, using a combination of a preconditioning step and cyclical time-stepping, achieve robust samples from a similar SGHMC scheme.  

The variational inference algorithm also does not outperform the regular SGD for model accuracy. 
On the other hand, the VI algorithms lead to the most well-calibrated models across every method. 
However, VI may also be much slower to train due to possibly requiring some extra forward passes.
Combined with the poorer predictive performance, the VI algorithm as implemented seems like an unattractive option.

In order to keep the modeling approach as purely Bayesian as possible, we are also giving up some demonstrably effective techniques, especially for fitting deeper models, such as batch normalization.
Incorporating these techniques into a Bayesian framework or possibly relaxing the requirement of a proper Bayesian posterior may also be necessary to achieve excellent performance.
For instance, \autocite{wenzel_how_2020} discusses how using a "cold" posterior, effectively sharpening the posterior distribution, seems to result in better performance than the proper Bayesian posterior.


\chapter{Conclusion}

Using small-scale simulated examples, we demonstrate that the SGHMC algorithm can improve sampling accuracy in a batched data setting compared to regular HMC.
We also show that including a gradient variance estimator improved the sampling performance in these simulated scenarios.
We also implement a variational inference algorithm.

However, we find that using these methods, a fully Bayesian approach to training deep learning models does not necessarily improve model performance.

We see performance improvements for a medium-sized convolutional model on the CIFAR10 dataset but worse performance when training a feed-forward network on MNIST and the DenseNet-121 model on the CIFAR10 dataset.

Taking a purely Bayesian approach to model estimation, we find that we cannot use specific modeling techniques that have proved helpful for training deep models. 
We also find diagnostics indicating that the sampling algorithms, as implemented, may yield inaccurate posterior samples.
Therefore, we must improve the robustness of the samplers before we can draw any definitive conclusions on the usefulness of a Bayesian MCMC approach in deep learning.

Using variational inference resulted in the best-calibrated models; however, the predictive performance was generally inferior to the other methods implemented.
